{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis (EDA) for Optimizing Manufacturing Yield\n",
    "**Project**: Optimizing Manufacturing Yield  \n",
    "**Dataset**: Intelligent Manufacturing Dataset (Kaggle)  \n",
    "**Objective**: Analyze sensor data to understand factors affecting yield, guiding optimization for manufacturing production settings. \n",
    " \n",
    "**Author**: Victoria Rios Vazquez (victoria.rios.vzz@gmail.com)\n",
    "\n",
    "**Date**: April 2025  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline\n",
    "sns.set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's import it from kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "data_path = \"../data/raw/manufacturing_6G_dataset.csv\"  # Adjust if filename differs\n",
    "df = pd.read_csv(data_path, parse_dates=[\"Timestamp\"])\n",
    "\n",
    "# Preview\n",
    "print(\"Dataset Shape:\", df.shape)\n",
    "print(\"\\nFirst 5 Rows:\")\n",
    "display(df.head())\n",
    "print(\"\\nColumns:\", df.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's fix the data type of the Machine IDs as they should be categories, not numeric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Machine_ID'] = df['Machine_ID'].astype('category')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Basic Data Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check data types, missing values, and summary stats, like summarizing omics features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data info\n",
    "print(\"\\nData Info:\")\n",
    "df.info()\n",
    "\n",
    "# Missing values\n",
    "print(\"\\nMissing Values:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Summary statistics\n",
    "print(\"\\nSummary Statistics:\")\n",
    "display(df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusions:** It seems like there are no missing values in the dataset, therefore techniques such as imputation will not be required in downstream analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Target Analysis: Yield\n",
    "As we saw in the preview of the dataset, we have a few columns that could be used as our target for yield efficiency, some of these are:\n",
    "-  **Quality_Control_Defect_Rate_%**, **Production_Speed_units_per_hr** or **Error_Rate_%**(continuous) as the target is continuous and directly tie to yield (lower defects = higher yield & higher production = higher yield) \n",
    "-  **Efficiency_Status** (categorical) this target could be useful for a classification problem, but if chosen could be a bit arbitrary (what counts as high, medium, and low efficiency? Not quantifiable = more difficult to improve).\n",
    "\n",
    "Therefore, we will choose the **Quality_Control_Defect_Rate_%**, **Production_Speed_units_per_hr** or **Error_Rate_%** variable as our target for yield efficiency of this process. Let's look at its distribution and whether or not it is suitable for this problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boxplot\n",
    "plt.figure(figsize=(18, 5))  # Wider for 3 plots in a row\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "sns.boxplot(data=df, x=\"Efficiency_Status\", y=\"Quality_Control_Defect_Rate_%\", palette=\"Set2\")\n",
    "plt.title(\"Defect Rate by Efficiency Status\")\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "sns.boxplot(data=df, x=\"Efficiency_Status\", y=\"Error_Rate_%\", palette=\"Set2\")\n",
    "plt.title(\"Error Rate by Efficiency Status\")\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "sns.boxplot(data=df, x=\"Efficiency_Status\", y=\"Production_Speed_units_per_hr\", palette=\"Set2\")\n",
    "plt.title(\"Production Speed by Efficiency Status\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Relationship Between Variables**\n",
    "\n",
    "It seems like *Error_Rate_%* and *Production_Speed_units_per_hr* show a stronger relationship with *Efficiency_Status* based on the visualizations above.\n",
    "\n",
    "To combine both speed and error into one performance indicator, we define a new variable:\n",
    "\n",
    "`Yield_Score = Production_Speed_units_per_hr * (1 - Error_Rate_% / 100)`\n",
    "\n",
    "**Interpretation**\n",
    "\n",
    "(1 - Error_Rate_% / 100): Fraction of error-free units\n",
    "→ e.g., if Error_Rate_% = 5%, then error-free rate = 0.95\n",
    "\n",
    "Multiplied by Production_Speed_units_per_hr:\n",
    "→ Estimates the number of defect-free units produced per hour\n",
    "\n",
    "E.g. a Yield_Score == 300, means the process is expected to yield 380 defect-free units per hour.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's add the new variable to the dataset\n",
    "df[\"Yield_Score\"] = df[\"Production_Speed_units_per_hr\"] * (1 - df[\"Error_Rate_%\"] / 100)\n",
    "\n",
    "# Let's plot it agains the other variables\n",
    "plt.figure(figsize=(24, 5))  # Wider for 3 plots in a row\n",
    "\n",
    "plt.subplot(1, 4, 1)\n",
    "sns.boxplot(data=df, x=\"Efficiency_Status\", y=\"Quality_Control_Defect_Rate_%\", palette=\"Set2\")\n",
    "plt.title(\"Defect Rate by Efficiency Status\")\n",
    "\n",
    "plt.subplot(1, 4, 2)\n",
    "sns.boxplot(data=df, x=\"Efficiency_Status\", y=\"Error_Rate_%\", palette=\"Set2\")\n",
    "plt.title(\"Error Rate by Efficiency Status\")\n",
    "\n",
    "plt.subplot(1, 4, 3)\n",
    "sns.boxplot(data=df, x=\"Efficiency_Status\", y=\"Production_Speed_units_per_hr\", palette=\"Set2\")\n",
    "plt.title(\"Production Speed by Efficiency Status\")\n",
    "\n",
    "plt.subplot(1, 4, 4)\n",
    "sns.boxplot(data=df, x=\"Efficiency_Status\", y=\"Yield_Score\", palette=\"Set2\")\n",
    "plt.title(\"Yield Score by Efficiency Status\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Feature Analysis\n",
    "Let's explore the sensor features (e.g., temperature, pressure) to identify patterns, like features-yield correlations and distributions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore Numeric Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Distributions**\n",
    "\n",
    "Let's explore the distributions of the numerical features, this will help us to detect their spread, range, and potential outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select only numerical columns\n",
    "num_cols = df.select_dtypes(include=np.number).columns\n",
    "\n",
    "# Define number of columns/rows for subplots\n",
    "n_cols = 3\n",
    "n_rows = int(np.ceil(len(num_cols) / n_cols))\n",
    "\n",
    "# Set figure size\n",
    "plt.figure(figsize=(5 * n_cols, 4 * n_rows))\n",
    "\n",
    "# Loop through numerical columns and plot\n",
    "for i, col in enumerate(num_cols, 1):\n",
    "    plt.subplot(n_rows, n_cols, i)\n",
    "    sns.histplot(df[col].dropna(), kde=True, bins=30, color='skyblue')\n",
    "    plt.title(col)\n",
    "    plt.xlabel('')\n",
    "    plt.ylabel('')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also look at the boxplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Select numerical columns\n",
    "num_cols = df.select_dtypes(include='number').columns.drop('Efficiency_Status', errors='ignore')\n",
    "\n",
    "# Melt to long format\n",
    "df_melted = df.melt(id_vars=\"Efficiency_Status\", value_vars=num_cols,\n",
    "                    var_name=\"Feature\", value_name=\"Value\")\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(15, 6))\n",
    "sns.boxplot(data=df_melted, x=\"Feature\", y=\"Value\", hue=\"Efficiency_Status\", palette=\"Set2\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.title(\"Boxplot of Numerical Features by Efficiency Status\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distributions of the numerical features appear to be uniform, this is not ideal for training a model as most assume normal distribution. For uniform distributions, it is recommended to apply quantile transformation (rank-based gaussianization) to approximate a normal distribution while it is reversible with the function .inverse_transform()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "\n",
    "# Select only numerical columns\n",
    "num_cols = df.select_dtypes(include=np.number).columns\n",
    "\n",
    "# Apply QuantileTransformer to all numerical columns\n",
    "qt = QuantileTransformer(output_distribution='normal', random_state=0)\n",
    "df_transformed = df.copy()\n",
    "df_transformed[num_cols] = qt.fit_transform(df[num_cols])\n",
    "\n",
    "# Define number of columns/rows for subplots\n",
    "n_cols = 3\n",
    "n_rows = int(np.ceil(len(num_cols) / n_cols))\n",
    "\n",
    "# Set figure size\n",
    "plt.figure(figsize=(5 * n_cols, 4 * n_rows))\n",
    "\n",
    "# Loop through numerical columns and plot\n",
    "for i, col in enumerate(num_cols, 1):\n",
    "    plt.subplot(n_rows, n_cols, i)\n",
    "    sns.histplot(df_transformed[col].dropna(), kde=True, bins=30, color='skyblue')\n",
    "    plt.title(col)\n",
    "    plt.xlabel('')\n",
    "    plt.ylabel('')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the features appear normally distributed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Correlations**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the correlation of the numeric features including the target features (defect rate) to observe any patterns (e.g. if temperature goes up, yield goes up) and potential predictors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select numeric columns (exclude non-numeric, adjust if needed)\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(df[num_cols].corr(), annot=True, cmap=\"coolwarm\", fmt=\".2f\")\n",
    "plt.title(\"Correlation with Yield Score\")\n",
    "plt.savefig(\"../docs/eda_plot.png\")\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nCorrelations with Yield Score:\")\n",
    "print(df_transformed[num_cols].corr()[\"Yield_Score\"].sort_values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also look at the scatterplots for these features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Original Distributions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define number of columns/rows for subplots\n",
    "n_cols = 3\n",
    "n_rows = int(np.ceil(len(num_cols) / n_cols))\n",
    "\n",
    "# Set figure size\n",
    "plt.figure(figsize=(5 * n_cols, 4 * n_rows))\n",
    "\n",
    "# Loop through numerical columns and plot\n",
    "for i, col in enumerate(num_cols, 1):\n",
    "    plt.subplot(n_rows, n_cols, i)\n",
    "    sns.scatterplot(x=col, y=\"Yield_Score\", hue=df[\"Efficiency_Status\"], data=df)\n",
    "    plt.title(f\"{col} vs. Yield Score\")\n",
    "    plt.xlabel(str(col))\n",
    "    plt.ylabel(\"Yield_Score\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformed Distributions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define number of columns/rows for subplots\n",
    "n_cols = 3\n",
    "n_rows = int(np.ceil(len(num_cols) / n_cols))\n",
    "\n",
    "# Set figure size\n",
    "plt.figure(figsize=(5 * n_cols, 4 * n_rows))\n",
    "\n",
    "# Loop through numerical columns and plot\n",
    "for i, col in enumerate(num_cols, 1):\n",
    "    plt.subplot(n_rows, n_cols, i)\n",
    "    sns.scatterplot(x=col, y=\"Yield_Score\", hue=df_transformed[\"Efficiency_Status\"], data=df_transformed)\n",
    "    plt.title(f\"{col} vs. Yield Score\")\n",
    "    plt.xlabel(str(col))\n",
    "    plt.ylabel(\"Yield Score\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusions:** It seems like there are already some insights about the features' contributions to the target variable. The scatter plots confirm the need to drop (leakage, strong/weak trends due to Yield_Score’s definition):\n",
    "- *Production_Speed_units_per_hr*\n",
    "- *Error_Rate_%* \n",
    "\n",
    "We might keep more informative parameters such as *Temperature_C*, *Vibration_Hz*, *Quality_Control_Defect_Rate_%*, etc., but address weak relationships with feature engineering (e.g., *Temperature_C* * *Vibration_Hz*) and non-linear models (Random Forest, MLP)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keep_numerical = ['Temperature_C', 'Vibration_Hz', 'Power_Consumption_kW',\n",
    "                  'Network_Latency_ms', 'Packet_Loss_%', 'Quality_Control_Defect_Rate_%', \n",
    "                  'Predictive_Maintenance_Score', 'Yield_Score']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorical Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's observe the same but for the categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select numeric columns (exclude non-numeric and date times, adjust if needed)\n",
    "categorical_cols = ['Operation_Mode', 'Efficiency_Status']\n",
    "\n",
    "for col in categorical_cols:\n",
    "\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    sns.boxplot(x=str(col), y=\"Yield_Score\", data=df_transformed, hue=\"Efficiency_Status\")\n",
    "    plt.title(f\"Yield Score by {col}\")\n",
    "    plt.savefig(f\"../docs/mode_yield_score_{col}.png\")\n",
    "    plt.show()\n",
    "    print(f\"\\nYield Score by {col}:\")\n",
    "    print(df.groupby(col)[\"Yield_Score\"].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Temporal Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's explore the temporal data we have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In notebooks/01_eda.ipynb\n",
    "print(\"Timestamp Range:\")\n",
    "print(\"Start:\", df[\"Timestamp\"].min())\n",
    "print(\"End:\", df[\"Timestamp\"].max())\n",
    "print(\"Sample Timestamps:\")\n",
    "print(df[\"Timestamp\"].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems all data comprises the first three months of 2024. Let's break down the components into hours, day of the week, date, is weekend, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features\n",
    "df[\"Hour\"] = df[\"Timestamp\"].dt.hour.astype(int)\n",
    "df[\"Day_of_Week\"] = df[\"Timestamp\"].dt.dayofweek.astype(int)  # 0=Mon, 6=Sun\n",
    "df[\"Date\"] = df[\"Timestamp\"].dt.date\n",
    "df[\"Is_Weekend\"] = df[\"Day_of_Week\"].isin([5, 6]).astype(int)  # 1=Weekend, 0=Weekday\n",
    "print(\"Sample Temporal Features:\")\n",
    "print(df[[\"Timestamp\", \"Hour\", \"Day_of_Week\", \"Date\", \"Is_Weekend\"]].tail(n=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize the temporal trends between the hour and yield score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hourly trend (fixed)\n",
    "plt.figure(figsize=(10, 5))\n",
    "# Group by Hour, average only Yield_Score\n",
    "hourly_yield = df.groupby(\"Hour\")[[\"Yield_Score\"]].mean().reset_index()\n",
    "sns.lineplot(x=\"Hour\", y=\"Yield_Score\", data=hourly_yield)\n",
    "plt.title(\"Average Yield_Score by Hour\")\n",
    "plt.savefig(\"../docs/yield_score_by_hour.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Daily trend:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Daily trend\n",
    "plt.figure(figsize=(12, 5))\n",
    "daily_yield = df.groupby(\"Date\")[\"Yield_Score\"].mean().reset_index()\n",
    "sns.lineplot(x=\"Date\", y=\"Yield_Score\", data=daily_yield)\n",
    "plt.title(\"Average Yield_Score by Date\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.savefig(\"../docs/yield_score_by_date.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Day of Week:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Day of Week trend\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.boxplot(x=\"Day_of_Week\", y=\"Yield_Score\", data=df)\n",
    "plt.title(\"Yield_Score by Day of Week\")\n",
    "plt.savefig(\"../docs/yield_score_by_dayofweek.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yield score by shift:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define shifts\n",
    "df[\"Shift\"] = pd.cut(df[\"Hour\"], bins=[0, 8, 16, 24], labels=[\"Night\", \"Morning\", \"Evening\"], right=False)\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.boxplot(x=\"Shift\", y=\"Yield_Score\", data=df)\n",
    "plt.title(\"Yield_Score by Shift\")\n",
    "plt.savefig(\"../docs/yield_score_by_shift.png\")\n",
    "plt.show()\n",
    "print(\"Yield_Score by Shift:\")\n",
    "print(df.groupby(\"Shift\")[\"Yield_Score\"].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation\n",
    "temporal_cols = [\"Hour\", \"Day_of_Week\", \"Is_Weekend\", \"Yield_Score\"]\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(df[temporal_cols].corr(), annot=True, cmap=\"coolwarm\", fmt=\".2f\")\n",
    "plt.title(\"Correlation with Yield_Score (Temporal Features)\")\n",
    "plt.savefig(\"../docs/temporal_corr_plot.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's prepare the temporal data for modelling, hours and days are cyclic (e.g., 23 → 0). Let's use sine/cosine to encode:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cyclic encoding for Hour\n",
    "df[\"Hour_sin\"] = np.sin(2 * np.pi * df[\"Hour\"] / 24)\n",
    "df[\"Hour_cos\"] = np.cos(2 * np.pi * df[\"Hour\"] / 24)\n",
    "df[\"Day_of_Week_sin\"] = np.sin(2 * np.pi * df[\"Day_of_Week\"] / 7)\n",
    "df[\"Day_of_Week_cos\"] = np.cos(2 * np.pi * df[\"Day_of_Week\"] / 7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the categorical and temporal features, we will drop:\n",
    "- Date, Hour and Day_of_Week (replaced by cyclic encoding)\n",
    "- Timestamp\n",
    "- Efficiency_Status (information leak as it is directly defining the target variable)\n",
    "\n",
    "We will keep:\n",
    "- Hour_sin\n",
    "- Hour_cos\n",
    "- Day_of_Week_sin\n",
    "- Day_of_Week_cos\n",
    "- Is_Weekend\n",
    "- Operation_Mode\n",
    "- Shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keep_categorical_and_temporal = ['Operation_Mode', 'Hour_sin', 'Hour_cos', 'Day_of_Week_sin', \n",
    "                                 'Day_of_Week_cos', 'Is_Weekend', 'Shift']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Outlier Detection\n",
    "Identify outliers in yield and sensors using the IQR method. The IQR method is simple, robust, and widely used for outlier detection in regression problems. It flags values below Q1 - 1.5IQR or above Q3 + 1.5IQR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In notebooks/01_eda.ipynb\n",
    "# Outlier detection using IQR\n",
    "def detect_outliers_iqr(df, columns):\n",
    "    outliers = pd.DataFrame()\n",
    "    for col in columns:\n",
    "        Q1 = df[col].quantile(0.25)\n",
    "        Q3 = df[col].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "        # Identify outliers\n",
    "        col_outliers = df[(df[col] < lower_bound) | (df[col] > upper_bound)]\n",
    "        outliers = pd.concat([outliers, col_outliers], axis=0).drop_duplicates()\n",
    "        print(f\"{col} - Outliers: {len(col_outliers)} rows, Lower Bound: {lower_bound:.2f}, Upper Bound: {upper_bound:.2f}\")\n",
    "    return outliers\n",
    "\n",
    "# Select numeric columns (excluding categorical and dropped columns)\n",
    "numeric_cols = [\"Temperature_C\", \"Vibration_Hz\", \"Power_Consumption_kW\", \n",
    "                \"Network_Latency_ms\", \"Packet_Loss_%\", \"Quality_Control_Defect_Rate_%\", \n",
    "                \"Predictive_Maintenance_Score\", \"Yield_Score\", \n",
    "                \"Hour\", \"Is_Weekend\", \"Hour_sin\", \"Hour_cos\", \n",
    "                \"Day_of_Week_sin\", \"Day_of_Week_cos\"]\n",
    "\n",
    "# Detect outliers\n",
    "outliers = detect_outliers_iqr(df, numeric_cols)\n",
    "print(\"\\nTotal Unique Outlier Rows:\", len(outliers))\n",
    "print(\"Outlier Sample:\")\n",
    "print(outliers[numeric_cols].head())\n",
    "\n",
    "# Optional: Remove outliers (uncomment to apply)\n",
    "# df_clean = df[~df.index.isin(outliers.index)]\n",
    "# print(\"Rows after removing outliers:\", len(df_clean))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are no outliers in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Yield_Score\"].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Initial Insights\n",
    "- **Yield**: Ranges 42.53–499.44, uniform distribution.\n",
    "- **Sensors**: No strong correlations with the target variable, uniform distribution.\n",
    "- **Issues**: No missing values, no outliers, maybe distribution is too uniform and there is not a strong \n",
    "  correlation with the target variable.\n",
    "- **Next Steps**: Normalize features (probably quantile transformation), model yield with multiple models MLP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Save Cleaned Data\n",
    "Export cleaned dataset for preprocessing, like saving processed omics data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the columns to keep\n",
    "keep_cols = keep_categorical_and_temporal + keep_numerical\n",
    "\n",
    "# Let's encode the categorical variables\n",
    "df_encoded = pd.get_dummies(df[keep_cols], columns=['Operation_Mode', 'Shift'], drop_first=False)\n",
    "\n",
    "# Convert boolean columns to integers (0 and 1)\n",
    "df_encoded[[\n",
    "    'Operation_Mode_Active', \n",
    "    'Operation_Mode_Idle', \n",
    "    'Operation_Mode_Maintenance', \n",
    "    'Shift_Night', \n",
    "    'Shift_Morning', \n",
    "    'Shift_Evening'\n",
    "]] = df_encoded[[\n",
    "    'Operation_Mode_Active', \n",
    "    'Operation_Mode_Idle', \n",
    "    'Operation_Mode_Maintenance', \n",
    "    'Shift_Night', \n",
    "    'Shift_Morning', \n",
    "    'Shift_Evening'\n",
    "]].astype(int)\n",
    "\n",
    "# View the encoded dataset\n",
    "df_encoded.head()\n",
    "df_encoded.dtypes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Export the clean dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean = df_encoded\n",
    "df_clean.to_csv(\"../data/clean/clean_encoded_df_target_yield.csv\", index=False)\n",
    "print(\"Saved cleaned data to ../data/clean/clean_encoded_df_target_yield.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
